{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empathy Emotion and Personality Detection using Deep Learning\n",
    "### 7120CEM CW2\n",
    "WASSA 2023 Shared Task on Empathy Emotion and Personality Detection in Interactions (* \n",
    "Including regression problems and classification problems):  \n",
    "- Website: https://codalab.lisn.upsaclay.fr/competitions/11167 \n",
    "- Summary paper: https://aclanthology.org/2023.wassa-1.44/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AttahiruJibril\\AppData\\Local\\Temp\\ipykernel_10168\\1535176026.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load data, preprocess and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>EmotionalPolarity</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Empathy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel very sad for the people.               ...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's terrible. Not only the people but the ani...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>3.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I felt really sorry for the sister that now ha...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.6667</td>\n",
       "      <td>2.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yeah, it's going to be tough but i am sure she...</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah, we never know what we can do unless we a...</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>2.3333</td>\n",
       "      <td>1.3333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  EmotionalPolarity  \\\n",
       "0  I feel very sad for the people.               ...             2.0000   \n",
       "1  It's terrible. Not only the people but the ani...             2.0000   \n",
       "2  I felt really sorry for the sister that now ha...             2.0000   \n",
       "3  Yeah, it's going to be tough but i am sure she...             0.6667   \n",
       "4  Yeah, we never know what we can do unless we a...             0.3333   \n",
       "\n",
       "   Emotion  Empathy  \n",
       "0   3.0000   3.3333  \n",
       "1   4.0000   3.3333  \n",
       "2   3.6667   2.6667  \n",
       "3   3.0000   2.0000  \n",
       "4   2.3333   1.3333  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'data/WASSA23_conv_level_with_labels_train.tsv'\n",
    "df = pd.read_table(data, header=0)\n",
    "new_col = []\n",
    "for names in df.columns:\n",
    "    new_col.append(names.strip())\n",
    "df.columns = new_col\n",
    "dataset = df.drop([\"conversation_id\", \"turn_id\", \"speaker_number\", \"article_id\", \"speaker_id\", \"essay_id\"], axis=1, inplace=True)\n",
    "\n",
    "X_data, y_data = df.loc[:, 'text'], df.drop('text', axis=1)\n",
    "X_train, X_test, y_train , y_test = train_test_split(X_data, y_data, train_size=0.8)\n",
    "#reset index of training examples\n",
    "X_train, X_test = X_train.reset_index(drop=True), X_test.reset_index(drop=True)\n",
    "y_train, y_test = y_train.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. process word data into numbers\n",
    "- tokenization\n",
    "- remove stop word and punctuatuons, numbers\n",
    "- lematization\n",
    "- vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocesses a sentence for natural language processing tasks.\n",
    "\n",
    "This function performs the following steps:\n",
    "    1. Tokenizes the sentence into individual words.\n",
    "    2. Removes stop words (common words with little meaning) from the tokens.\n",
    "    3. Removes punctuation marks from the tokens.\n",
    "    4. Lemmatizes the tokens (reduces words to their base form).\n",
    "    5. Joins the preprocessed tokens back into a sentence string.\n",
    "\n",
    "Args:\n",
    "\tsentence: The input sentence to be preprocessed (string).\n",
    "\n",
    "Returns:\n",
    "\tThe preprocessed sentence string.\n",
    "\"\"\"\n",
    "def word_preprocessor(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuations = set(string.punctuation)\n",
    "    lem = WordNetLemmatizer().lemmatize\n",
    "    sentence = word_tokenize(sentence)\n",
    "    sentence = [word for word in sentence if word not in stop_words]\n",
    "    sentence = [word for word in sentence if word not in punctuations]\n",
    "    sentence_str = ' '.join(sentence)\n",
    "    sentence = lem(sentence_str)\n",
    "    return sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(word_preprocessor)\n",
    "X_test = X_test.apply(word_preprocessor)\n",
    "\n",
    "#convert labels to array\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train[['EmotionalPolarity', 'Emotion', 'Empathy']]), np.array(y_test[['EmotionalPolarity', 'Emotion', 'Empathy']])\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=8000, stop_words='english', lowercase=True)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "X_train_vec = X_train_vec.toarray()\n",
    "X_test_vec = X_test_vec.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AttahiruJibril\\.conda\\envs\\ML-AI\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 80ms/step - accuracy: 0.5324 - loss: 2.9654 - val_accuracy: 0.5467 - val_loss: 2.8716\n",
      "Epoch 2/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 86ms/step - accuracy: 0.6669 - loss: 2.9259 - val_accuracy: 0.5404 - val_loss: 2.8755\n",
      "Epoch 3/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 90ms/step - accuracy: 0.7351 - loss: 2.8742 - val_accuracy: 0.5467 - val_loss: 2.8742\n",
      "Epoch 4/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 86ms/step - accuracy: 0.8029 - loss: 2.8630 - val_accuracy: 0.5450 - val_loss: 2.8766\n",
      "Epoch 5/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 88ms/step - accuracy: 0.8179 - loss: 2.8993 - val_accuracy: 0.5444 - val_loss: 2.8760\n",
      "Epoch 6/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 75ms/step - accuracy: 0.8397 - loss: 2.7785 - val_accuracy: 0.5359 - val_loss: 2.8782\n",
      "Epoch 7/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 75ms/step - accuracy: 0.8396 - loss: 2.8060 - val_accuracy: 0.5319 - val_loss: 2.8808\n",
      "Epoch 8/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - accuracy: 0.8535 - loss: 2.8915 - val_accuracy: 0.5347 - val_loss: 2.8791\n",
      "Epoch 9/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 79ms/step - accuracy: 0.8462 - loss: 2.9020 - val_accuracy: 0.5513 - val_loss: 2.8767\n",
      "Epoch 10/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 79ms/step - accuracy: 0.8565 - loss: 2.8455 - val_accuracy: 0.5353 - val_loss: 2.8811\n",
      "Epoch 11/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 78ms/step - accuracy: 0.8549 - loss: 2.8635 - val_accuracy: 0.5296 - val_loss: 2.8816\n",
      "Epoch 12/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 80ms/step - accuracy: 0.8558 - loss: 2.8473 - val_accuracy: 0.5382 - val_loss: 2.8794\n",
      "Epoch 13/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 80ms/step - accuracy: 0.8483 - loss: 2.8818 - val_accuracy: 0.5376 - val_loss: 2.8795\n",
      "Epoch 14/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 79ms/step - accuracy: 0.8649 - loss: 2.8553 - val_accuracy: 0.5513 - val_loss: 2.8796\n",
      "Epoch 15/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 78ms/step - accuracy: 0.8603 - loss: 2.8440 - val_accuracy: 0.5199 - val_loss: 2.8809\n",
      "Epoch 16/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 78ms/step - accuracy: 0.8662 - loss: 2.8127 - val_accuracy: 0.5461 - val_loss: 2.8828\n",
      "Epoch 17/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 78ms/step - accuracy: 0.8567 - loss: 2.8798 - val_accuracy: 0.5433 - val_loss: 2.8819\n",
      "Epoch 18/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 77ms/step - accuracy: 0.8609 - loss: 2.8256 - val_accuracy: 0.5404 - val_loss: 2.8842\n",
      "Epoch 19/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 79ms/step - accuracy: 0.8574 - loss: 2.8600 - val_accuracy: 0.5256 - val_loss: 2.8826\n",
      "Epoch 20/20\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 76ms/step - accuracy: 0.8468 - loss: 2.8473 - val_accuracy: 0.5376 - val_loss: 2.8866\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5297 - loss: 2.8741\n",
      "|Test loss: \t2.8891\n",
      "|Test accuracy: \t0.5376\n"
     ]
    }
   ],
   "source": [
    "# Create a sequential model\n",
    "EEPD_Model = Sequential()\n",
    "EEPD_Model.add(Dense(800, activation='relu', input_dim=X_train_vec.shape[1]))\n",
    "EEPD_Model.add(Dense(400, activation='relu'))\n",
    "EEPD_Model.add(Dense(200, activation='relu'))\n",
    "EEPD_Model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "EEPD_Model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "EEPD_Model.fit(X_train_vec, y_train, epochs=20, batch_size=128, validation_data=(X_test_vec, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = EEPD_Model.evaluate(X_test_vec, y_test)\n",
    "print(f'|Test loss: \\t{loss:.4f}')\n",
    "print(f'|Test accuracy: \\t{accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
